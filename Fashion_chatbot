!pip install datasets transformers
import os
os.environ["WANDB_DISABLED"] = "true"
os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"

import pandas as pd
import torch
from datasets import Dataset, DatasetDict
from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments
# Load and clean dataset
df = pd.read_csv("/content/Bitext_Sample_Customer_Support_Training_Dataset_27K_responses-v11.csv")
df.head()
# missing values
df.isnull().sum()
# Randomly select 1000 rows
df_sample = df.sample(n=1000, random_state=42)


# Clean dataset
df_sample = df_sample.dropna(subset=["instruction", "response"])
df_sample = df_sample[df_sample["instruction"].apply(lambda x: isinstance(x, str) and len(x.strip()) > 0)]
df_sample = df_sample[df_sample["response"].apply(lambda x: isinstance(x, str) and len(x.strip()) > 0)]
dataset = Dataset.from_pandas(df_sample.reset_index(drop=True))

# Load tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("distilgpt2")
tokenizer.pad_token = tokenizer.eos_token

# Preprocessing function
def preprocess_function(examples):
    combined = [instr + " " + resp for instr, resp in zip(examples["instruction"], examples["response"])]
    return tokenizer(combined, truncation=True, padding="max_length", max_length=512)

# Tokenize dataset
tokenized_dataset = dataset.map(preprocess_function, batched=True)
tokenized_dataset = tokenized_dataset.remove_columns(["instruction", "response", "category", "intent", "flags"])
tokenized_dataset = DatasetDict({"train": tokenized_dataset})

def to_torch(example):
    input_ids = torch.tensor(example["input_ids"])
    attention_mask = torch.tensor(example["attention_mask"])

    example["input_ids"] = input_ids
    example["attention_mask"] = attention_mask
    example["labels"] = input_ids.clone()  # Labels for causal LM

    return example

# Manual tensor conversion
tokenized_dataset["train"] = tokenized_dataset["train"].map(to_torch)

# Load model
model = GPT2LMHeadModel.from_pretrained("distilgpt2")

# Training settings
training_args = TrainingArguments(
    output_dir="/content/drive/MyDrive/results",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    save_steps=500,
    save_total_limit=2,
    logging_steps=100,
    logging_dir="/content/drive/MyDrive/logs",
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
)

# Start training
trainer.train()

# Save model
model.save_pretrained("/content/drive/MyDrive/fashion_bot_model")
tokenizer.save_pretrained("/content/drive/MyDrive/fashion_bot_model")
